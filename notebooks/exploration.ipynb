{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadaadf5",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Data Exploration & Analysis\n",
    "\n",
    "This notebook provides comprehensive data exploration and analysis for the heart disease prediction dataset.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Overview](#data-loading)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Feature Analysis](#feature-analysis)\n",
    "4. [Correlation Analysis](#correlation)\n",
    "5. [Data Preprocessing](#preprocessing)\n",
    "6. [Statistical Insights](#statistics)\n",
    "7. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Libraries imported successfully!\")\n",
    "print(\"üé® Plotting style configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade780e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Overview {#data-loading}\n",
    "\n",
    "Let's start by loading the heart disease dataset and getting an overview of the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../data/heart_disease_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Creating sample dataset...\")\n",
    "    \n",
    "    # Create sample dataset for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    data = {\n",
    "        'age': np.random.randint(30, 80, n_samples),\n",
    "        'sex': np.random.randint(0, 2, n_samples),\n",
    "        'cp': np.random.randint(0, 4, n_samples),\n",
    "        'trestbps': np.random.randint(90, 200, n_samples),\n",
    "        'chol': np.random.randint(120, 400, n_samples),\n",
    "        'fbs': np.random.randint(0, 2, n_samples),\n",
    "        'restecg': np.random.randint(0, 3, n_samples),\n",
    "        'thalach': np.random.randint(80, 200, n_samples),\n",
    "        'exang': np.random.randint(0, 2, n_samples),\n",
    "        'oldpeak': np.random.uniform(0, 6, n_samples),\n",
    "        'slope': np.random.randint(0, 3, n_samples),\n",
    "        'ca': np.random.randint(0, 4, n_samples),\n",
    "        'thal': np.random.randint(0, 4, n_samples),\n",
    "    }\n",
    "    \n",
    "    # Create target with some correlation\n",
    "    target_prob = (\n",
    "        0.1 * (data['age'] > 55) +\n",
    "        0.2 * (data['cp'] > 0) +\n",
    "        0.15 * (data['chol'] > 240) +\n",
    "        0.1 * (data['thalach'] < 120) +\n",
    "        0.15 * (data['exang'] == 1) +\n",
    "        0.1 * (data['oldpeak'] > 2) +\n",
    "        0.2 * np.random.random(n_samples)\n",
    "    )\n",
    "    \n",
    "    data['target'] = (target_prob > 0.5).astype(int)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"‚úÖ Sample dataset created: {df.shape}\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìã Dataset Info:\")\n",
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"\\nüìä Column Names:\")\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"üîç First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìä Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Total missing values: {missing_df['Missing Count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ea28d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis {#eda}\n",
    "\n",
    "Let's explore the distribution of our target variable and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "target_col = 'target'  # Adjust if your target column has a different name\n",
    "\n",
    "print(\"üéØ Target Variable Analysis:\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "target_counts = df[target_col].value_counts()\n",
    "target_percent = df[target_col].value_counts(normalize=True) * 100\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_percent\n",
    "})\n",
    "\n",
    "display(target_summary)\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x=target_col, ax=axes[0])\n",
    "axes[0].set_title('Target Variable Distribution (Count)')\n",
    "axes[0].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=['No Disease', 'Disease'], autopct='%1.1f%%')\n",
    "axes[1].set_title('Target Variable Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ac8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics\n",
    "print(\"üìä Descriptive Statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfac36f",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis {#feature-analysis}\n",
    "\n",
    "Let's analyze individual features and their relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d87a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numerical_features:\n",
    "    numerical_features.remove(target_col)\n",
    "\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"üìä Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"üìä Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Identify binary features (likely categorical despite being numeric)\n",
    "binary_features = []\n",
    "for col in numerical_features:\n",
    "    unique_vals = df[col].nunique()\n",
    "    if unique_vals <= 2:\n",
    "        binary_features.append(col)\n",
    "\n",
    "print(f\"üî¢ Binary features: {binary_features}\")\n",
    "\n",
    "# Continuous numerical features\n",
    "continuous_features = [col for col in numerical_features if col not in binary_features]\n",
    "print(f\"üìà Continuous features: {continuous_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of continuous features\n",
    "if continuous_features:\n",
    "    n_features = len(continuous_features)\n",
    "    cols = 3\n",
    "    rows = (n_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten() if n_features > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(continuous_features):\n",
    "        if i < len(axes):\n",
    "            axes[i].hist(df[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'Distribution of {feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Distribution of Continuous Features', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No continuous features found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of continuous features by target\n",
    "if continuous_features:\n",
    "    n_features = len(continuous_features)\n",
    "    cols = 3\n",
    "    rows = (n_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten() if n_features > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(continuous_features):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(data=df, x=target_col, y=feature, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature} by Target')\n",
    "            axes[i].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Feature Distributions by Target Variable', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical/Binary features analysis\n",
    "categorical_and_binary = categorical_features + binary_features\n",
    "\n",
    "if categorical_and_binary:\n",
    "    n_features = len(categorical_and_binary)\n",
    "    cols = 3\n",
    "    rows = (n_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten() if n_features > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(categorical_and_binary):\n",
    "        if i < len(axes):\n",
    "            # Create cross-tabulation\n",
    "            ct = pd.crosstab(df[feature], df[target_col], normalize='index') * 100\n",
    "            ct.plot(kind='bar', ax=axes[i], width=0.8)\n",
    "            axes[i].set_title(f'{feature} vs Target (%)')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Percentage')\n",
    "            axes[i].legend(['No Disease', 'Disease'])\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Categorical Features vs Target Variable', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1c0f8",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis {#correlation}\n",
    "\n",
    "Let's analyze correlations between features and with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ddc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target\n",
    "target_correlations = correlation_matrix[target_col].drop(target_col).sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"üéØ Features ranked by correlation with target:\")\n",
    "target_corr_df = pd.DataFrame({\n",
    "    'Feature': target_correlations.index,\n",
    "    'Correlation': target_correlations.values,\n",
    "    'Abs_Correlation': np.abs(target_correlations.values)\n",
    "})\n",
    "\n",
    "display(target_corr_df)\n",
    "\n",
    "# Plot target correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "target_correlations.plot(kind='barh')\n",
    "plt.title('Feature Correlations with Target Variable', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368663b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated feature pairs\n",
    "print(\"üîç Highly correlated feature pairs (|correlation| > 0.7):\")\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        feature1 = correlation_matrix.columns[i]\n",
    "        feature2 = correlation_matrix.columns[j]\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        if abs(corr_value) > 0.7:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': feature1,\n",
    "                'Feature 2': feature2,\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "    display(high_corr_df)\n",
    "else:\n",
    "    print(\"‚úÖ No highly correlated feature pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4dd98",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing {#preprocessing}\n",
    "\n",
    "Let's prepare the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be648bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and unique values\n",
    "print(\"üìä Data Type Analysis:\")\n",
    "dtype_info = pd.DataFrame({\n",
    "    'Feature': df.columns,\n",
    "    'Data_Type': df.dtypes,\n",
    "    'Unique_Values': [df[col].nunique() for col in df.columns],\n",
    "    'Sample_Values': [list(df[col].unique()[:5]) for col in df.columns]\n",
    "})\n",
    "\n",
    "display(dtype_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "print(\"üîç Outlier Analysis:\")\n",
    "outlier_summary = []\n",
    "\n",
    "for feature in continuous_features:\n",
    "    outliers = detect_outliers_iqr(df, feature)\n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Outlier_Count': len(outliers),\n",
    "        'Outlier_Percentage': (len(outliers) / len(df)) * 100\n",
    "    })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    display(outlier_df)\n",
    "else:\n",
    "    print(\"No continuous features for outlier analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d473aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling analysis\n",
    "if continuous_features:\n",
    "    print(\"üìä Feature Scaling Analysis:\")\n",
    "    \n",
    "    # Show statistics before scaling\n",
    "    print(\"\\nBefore Scaling:\")\n",
    "    scaling_stats = df[continuous_features].describe().loc[['mean', 'std', 'min', 'max']]\n",
    "    display(scaling_stats)\n",
    "    \n",
    "    # Apply standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[continuous_features])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=continuous_features)\n",
    "    \n",
    "    print(\"\\nAfter Standard Scaling:\")\n",
    "    scaling_stats_after = scaled_df.describe().loc[['mean', 'std', 'min', 'max']]\n",
    "    display(scaling_stats_after)\n",
    "    \n",
    "    # Visualize scaling effect\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Before scaling\n",
    "    df[continuous_features].boxplot(ax=axes[0])\n",
    "    axes[0].set_title('Before Scaling')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # After scaling\n",
    "    scaled_df.boxplot(ax=axes[1])\n",
    "    axes[1].set_title('After Standard Scaling')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Effect of Feature Scaling', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5408a55",
   "metadata": {},
   "source": [
    "## 6. Statistical Insights {#statistics}\n",
    "\n",
    "Let's perform statistical tests to understand feature significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for continuous features\n",
    "if continuous_features:\n",
    "    print(\"üìä Statistical Tests for Continuous Features:\")\n",
    "    print(\"(Testing difference between disease and no-disease groups)\")\n",
    "    \n",
    "    stat_results = []\n",
    "    \n",
    "    for feature in continuous_features:\n",
    "        # Separate groups\n",
    "        group_0 = df[df[target_col] == 0][feature]\n",
    "        group_1 = df[df[target_col] == 1][feature]\n",
    "        \n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(group_0, group_1)\n",
    "        \n",
    "        # Calculate effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(group_0) - 1) * group_0.var() + \n",
    "                             (len(group_1) - 1) * group_1.var()) / \n",
    "                            (len(group_0) + len(group_1) - 2))\n",
    "        cohens_d = (group_1.mean() - group_0.mean()) / pooled_std\n",
    "        \n",
    "        stat_results.append({\n",
    "            'Feature': feature,\n",
    "            'Mean_NoDisease': group_0.mean(),\n",
    "            'Mean_Disease': group_1.mean(),\n",
    "            'T_Statistic': t_stat,\n",
    "            'P_Value': p_value,\n",
    "            'Cohens_D': cohens_d,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "    \n",
    "    stat_df = pd.DataFrame(stat_results)\n",
    "    display(stat_df)\n",
    "    \n",
    "    # Interpretation of Cohen's d\n",
    "    print(\"\\nüìã Cohen's d interpretation:\")\n",
    "    print(\"‚Ä¢ Small effect: 0.2\")\n",
    "    print(\"‚Ä¢ Medium effect: 0.5\")\n",
    "    print(\"‚Ä¢ Large effect: 0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ea372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests for categorical features\n",
    "if categorical_and_binary:\n",
    "    print(\"üìä Chi-square Tests for Categorical Features:\")\n",
    "    \n",
    "    chi2_results = []\n",
    "    \n",
    "    for feature in categorical_and_binary:\n",
    "        # Create contingency table\n",
    "        contingency_table = pd.crosstab(df[feature], df[target_col])\n",
    "        \n",
    "        # Perform chi-square test\n",
    "        chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Calculate Cram√©r's V (effect size)\n",
    "        n = contingency_table.sum().sum()\n",
    "        cramers_v = np.sqrt(chi2_stat / (n * (min(contingency_table.shape) - 1)))\n",
    "        \n",
    "        chi2_results.append({\n",
    "            'Feature': feature,\n",
    "            'Chi2_Statistic': chi2_stat,\n",
    "            'P_Value': p_value,\n",
    "            'Degrees_of_Freedom': dof,\n",
    "            'Cramers_V': cramers_v,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "    \n",
    "    chi2_df = pd.DataFrame(chi2_results)\n",
    "    display(chi2_df)\n",
    "    \n",
    "    print(\"\\nüìã Cram√©r's V interpretation:\")\n",
    "    print(\"‚Ä¢ Small effect: 0.1\")\n",
    "    print(\"‚Ä¢ Medium effect: 0.3\")\n",
    "    print(\"‚Ä¢ Large effect: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59775030",
   "metadata": {},
   "source": [
    "## 7. Conclusions {#conclusions}\n",
    "\n",
    "Let's summarize our findings and provide recommendations for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"üìã DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"‚Ä¢ Total samples: {len(df)}\")\n",
    "print(f\"‚Ä¢ Total features: {len(df.columns) - 1}\")\n",
    "print(f\"‚Ä¢ Target distribution: {df[target_col].value_counts().to_dict()}\")\n",
    "print(f\"‚Ä¢ Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nüîç Feature Analysis:\")\n",
    "print(f\"‚Ä¢ Continuous features: {len(continuous_features)}\")\n",
    "print(f\"‚Ä¢ Binary features: {len(binary_features)}\")\n",
    "print(f\"‚Ä¢ Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Top correlated features with target\n",
    "if 'target_correlations' in locals():\n",
    "    top_features = target_correlations.head(3)\n",
    "    print(f\"\\nüéØ Top 3 features correlated with target:\")\n",
    "    for feature, corr in top_features.items():\n",
    "        print(f\"‚Ä¢ {feature}: {corr:.3f}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° RECOMMENDATIONS FOR MODEL BUILDING:\")\n",
    "print(f\"1. ‚úÖ Dataset is ready for machine learning\")\n",
    "print(f\"2. üîß Apply standard scaling for tree-based models\")\n",
    "print(f\"3. üìä Consider feature selection based on correlation analysis\")\n",
    "print(f\"4. üéØ Target classes are reasonably balanced\")\n",
    "print(f\"5. üîç Monitor for overfitting due to dataset size\")\n",
    "\n",
    "if 'high_corr_pairs' in locals() and high_corr_pairs:\n",
    "    print(f\"6. ‚ö†Ô∏è  Consider removing highly correlated features\")\n",
    "\n",
    "if 'outlier_df' in locals() and not outlier_df.empty:\n",
    "    outlier_features = outlier_df[outlier_df['Outlier_Percentage'] > 5]['Feature'].tolist()\n",
    "    if outlier_features:\n",
    "        print(f\"7. üîß Consider outlier treatment for: {outlier_features}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to proceed with model training!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
